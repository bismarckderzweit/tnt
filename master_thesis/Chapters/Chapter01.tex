 %************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************
\section{Interpretability and feature attribution}

Research on model interpretability has become a focus of attention in scientific research conferences in the past several years, because we are not only satisfied with the effect of the model, but also has more thinking about the reasons for the model decision. This kind of thinking helps to optimize the model and features, and it can help better understand the model itself and improve the quality of model services.
One of the most popular technique to achieve interpretability is to attribute the reason behind a certain decision to its input features and is called feature attribution. Feature attribution aims more specifically to characterize the response of a machine learning model by discovering which parts of the model input are most responsible for determining its output.  Feature attribution is performed post-hoc to model training in either a model introspective or model agnostic manner. Model introspective methods take model parameters and training algorithm into account whereas in the model agnostic setting only input and output is observable. Gradient based approaches for differentiable systems are a popular example here. In a model agnostic setting, feature attribution is often determined by approximating the original model by a simpler proxy model.

\section{Something missing in past literature}

Interpretability methods have recently been extended to ranking models learned from data such as click logs. This work mainly deals with textual features and ad-hoc retrieval tasks~\cite{Singhexs:2019, singh2020model, Fernando:2019:SIN:3331184.3331312}. Nevertheless, the pragmatic scenarios like web search, recommender systems, vertical search, etc operate with numerous features to work out a ranking list whereas textual features are only a a small subset. In such condition, Learning to Rank are applied, referred to as L2R or LTR for short, the core of LTR is still machine learning, but the goal is not just simple classification or regression, the most important thing is to sort the documents. While feature attribution approaches exist for standard ML models like neural networks and decision trees, they all fall under the model introspective category. In the model agnostic domain, SHAP~\cite{lundberg2017unified} and LIME~\cite{ribeiro2016i} are popular approaches devised specifically for classification and regression problems. 
However, The complexity of machine learning models makes previous papers rarely discuss that why the LTR model makes such a decision. The opacity of models becomes the bottleneck for LTR's evolution and troubleshooting. 
The feature attribution of the LTR model will first enable us to faithfully explain the output of the entire ranking, rather than explaining the score or classification results of individual items. Second, the model agnostic feature attribution method can help us cross-examine various classification algorithms without having to worry about training regimen (pointwise, pairwise, listwise), access to training data, and access to model parameters. This approach is useful not only for debugging by model developers, but also for auditing and legislative processes that strike a balance between exposing trade secrets and transparency.

\section{Thesis Goals}
The problem setting of feature attribution for LTR is as follows: given a trained model and a set of input items corresponding to a query or user, our aim is to select a subset of features, the attribution set, as an explanation that faithfully describes the ranking decision.
First, we characterize feature attributions of a given pre-trained LTR model in terms of their validity and completeness. Validity encodes the amount of predictive capacity contained in the explanatory features or attribution set. Completeness on the other hand measures the lack of information in the non-attributed set or the features that are not explanation features.
Note that an attribution set might be valid but not complete. Consider an example where the two variables encode the predictive capacity for a ranking decision and are perfectly correlated. In such a scenario, selecting any one of these variables would result in high validity. However, if only of these features is chosen into the attribution set the non-attribution set also retains enough predictive capacity. 
In this thesis we developed several categories of algorithms to explain learning to rank models, namely validity, completeness and alpha. Our methods identify a small subset of input features as explanation to a ranking decision. We have selected $\Delta$NDCG and Kendall's $\tau$ as our evaluation indicators for the explanation, namely normalized discounted cumulative gain and Kendall rank correlation coefficient, respectively.

A large number of quantitative experiments used 2 commonly LTR datasets -- MQ2008 and MSLR-WEB10K on a variety of LTR models have been set up, our experiments show that, in most cases, our feature attribution method is better than SHAP, a agnostic explanation approach, in terms of validity and completeness metrics.  

\section{Thesis Structure}
The remainder of this thesis is organized as follows: Chapter 2, provides a review of the background and the related work on both learning to rank and model explanation. Chapter 3, we make a short introduction of the features and make a more detailed definition of the problem. Chapter 4, we make a clear explanation of our novel approaches, the preference matrix and a Greedy Incremental Search algorithm were proprosed, in addition, the implement of shapley value way on explanation will also be mentioned. Within chapter 5, we present the setup of experiments, and show detailed results to analyse, we will make comparison with approached. In chapter 6 we discuss the results in depth and do more analysis, the result of alpha is revealed here. The conclusion and future enhancements are discussed in chapter 7, we summarize what we have achieved and where we need to improve.
