%************************************************
\chapter{Problem Defination}\label{ch:Problem Defination} % $\mathbb{ZNR}$
%************************************************
\subsection{Learning to Rank Features}
For a learning to rank task, the model handles features to make a rank list. For each query-document pair, it contains host of features, for instance, for dataset MQ2008 from LETOR 4.0~\cite{qin2013introducing}, which is a Supervised ranking dataset. Here are several example rows from MQ2008 dataset: 

\begin{lstlisting}
2 qid:10032 1:0.056537 2:0.000000 3:0.666667 4:1.000000 5:0.067138 ... 45:0.000000 46:0.076923 #docid = GX029-35-5894638 inc = 0.0119881192468859 prob = 0.139842 
0 qid:10032 1:0.279152 2:0.000000 3:0.000000 4:0.000000 5:0.279152 ... 45:0.250000 46:1.000000 #docid = GX030-77-6315042 inc = 1 prob = 0.341364 0 qid:10032 1:0.130742 2:0.000000 3:0.333333 4:0.000000 5:0.134276 ... 45:0.750000 46:1.000000 #docid = GX140-98-13566007 inc = 1 prob = 0.0701303 
1 qid:10032 1:0.593640 2:1.000000 3:0.000000 4:0.000000 5:0.600707 ... 45:0.500000 46:0.000000 #docid = GX256-43-0740276 inc = 0.0136292023050293 prob = 0.400738 

\end{lstlisting}
Each row represents a query-document pair. The first column is relevance label of this pair, the second column is query id, the following columns are features, and the end of the row is comment about the pair, including id of the document. The larger the relevance label, the more relevant the query-document pair. A query-document pair is represented by a 46-dimensional feature vector, they are show in table~\ref{tab:features}. Some features like the second one TF of anchor, the 39th LMIR.JM of URL, the 41th PageRank score, in common sense, they are important features. 
\begin{longtable}{|c|l|}
\hline
Column in Output & Description                             \\ \hline
1                & TF(Term frequency) of body              \\ \hline
2                & TF of anchor                            \\ \hline
3                & TF of title                             \\ \hline
4                & TF of URL                               \\ \hline
5                & TF of whole document                    \\ \hline
6                & IDF(Inverse document frequency) of body \\ \hline
7                & IDF of anchor                           \\ \hline
8                & IDF of title                            \\ \hline
9                & IDF of URL                              \\ \hline
10               & IDF of whole document                   \\ \hline
11               & TF*IDF of body                          \\ \hline
12               & TF*IDF of anchor                        \\ \hline
13               & TF*IDF of title                         \\ \hline
14               & TF*IDF of URL                           \\ \hline
15               & TF*IDF of whole document                \\ \hline
16               & DL(Document length) of body             \\ \hline
17               & DL of anchor                            \\ \hline
18               & DL of title                             \\ \hline
19               & DL of URL                               \\ \hline
20               & DL of whole document                    \\ \hline
21               & BM25 of body                            \\ \hline
22               & BM25 of anchor                          \\ \hline
23               & BM25 of title                           \\ \hline
24               & BM25 of URL                             \\ \hline
25               & BM25 of whole document                  \\ \hline
26               & LMIR.ABS of body                        \\ \hline
27               & LMIR.ABS of anchor                      \\ \hline
28               & LMIR.ABS of title                       \\ \hline
29               & LMIR.ABS of URL                         \\ \hline
30               & LMIR.ABS of whole document              \\ \hline
31               & LMIR.DIR of body                        \\ \hline
32               & LMIR.DIR of anchor                      \\ \hline
33               & LMIR.DIR of title                       \\ \hline
34               & LMIR.DIR of URL                         \\ \hline
35               & LMIR.DIR of whole document              \\ \hline
36               & LMIR.JM of body                         \\ \hline
37               & LMIR.JM of anchor                       \\ \hline
38               & LMIR.JM of title                        \\ \hline
39               & LMIR.JM of URL                          \\ \hline
40               & LMIR.JM of whole document               \\ \hline
41               & PageRank                                \\ \hline
42               & Inlink number                           \\ \hline
43               & Outlink number                          \\ \hline
44               & Number of slash in URL                  \\ \hline
45               & Length of URL                           \\ \hline
46               & Number of child page                    \\ \hline
\caption{46 features for LETOR 4.0 supervised ranking datasets}\label{tab:features}
\end{longtable}
The discription of another dataset MSLR-WEB10k is in appendix.

\subsection{Defining Explanations}



A famous work called Feature Selection \cite{geng2007feature} is a dimensionality reduction technique which aims at choosing a small subset of the relevant features from the original features by removing irrelevant, redundant or noisy features. Specifically, for each feature it uses its value to rank the training instances, and define the ranking accuracy in terms of a performance measure or a loss function as the importance of the feature. But if we change another view to select features, we could select a small subset of features independent of the learning model to satisfy the completeness and validity criteria as an explanation, where comes from our basic thought. 

Given a trained LTR model, $r(\f, \Phi(.)) \to \pi$ , we are interested in explaining the output ranking $\pi$ in terms of a subset of features $\f' \subseteq \f$ that are most \textit{impacting} for $\pi$, i.e., $r(\f', \Phi(.))$. We define two measures, namely \emph{validity} and \emph{completeness}, to quantify the attribution of a feature subset (explanation) for $\pi$.

\begin{definition}[Validity]We define an explanation $\f' \subset \f$ to be valid if $r(\f')\approx r(\f)$, i.e., the model's output is predictable from the explanation alone. In our setting a valid explanation implies that the returned smaller set of features are sufficient to reconstruct the original ranking output by the model when using all the features. In particular we measure \textit{validity score} ($v$) of our explanation by computing the rank correlation between the ranking returned by the model when using the explanation and the original ranking.
$$\mathcal{V}= \tau(r(\f'), r(\f)),$$
\end{definition}

where $\tau$ is the kendall rank correlation coefficient defined over a pair of rankings $\pi$, it ranges from -1 to 1, $\pi'$ such that
$$ \tau(\pi,\pi') = {\text{no. of concordant pairs}- \text{no. of discordant pairs} \over \text{total no. of pairs}} $$

\begin{definition}[Completeness]An explanation $\f'\subset\f $ is said to be complete with respect to a learned model $r$ if removing or altering the explanation features from the input will change the output function or ranking considerably. In other words, an explanation $\f'$ is complete if  $r(\f \setminus \f')$ cannot approximate $r(\f)$. We define completeness score of an explanation as the negative of rank correlation between the original ranking and the ranking obtained by using features which are not included in the explanation, i.e., 
$$ \mathcal{C}= -\tau(r(\f \setminus \f'), r(\f)).$$

\end{definition}

Intuitively, a feature attribution $\f'$ is \textit{valid} if it contains enough information to re-create $\pi$ when $\f / \f'$ is not considered for ranking.
Alternately $\f'$ is \textit{complete} if $\f / \f'$ does not contain enough information capacity to re-create the ranking $\pi$. Note that a valid attribution $\f'$ might not always be complete. This case arises, for example, when two or more predictive features are correlated, hence only one of them suffices to be in the valid set. In the following chapter we formulate the optimization problems for finding explanations and describe our approaches.









