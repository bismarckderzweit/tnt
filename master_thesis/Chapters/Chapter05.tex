%************************************************
\chapter{Experimental Setup}\label{ch:setup} % $\mathbb{ZNR}$
%************************************************
\section{Experiment Preparation and Implement}
\textbf{Datasets} We chose two different learning-to-rank datasets: \textsc{MQ2008} and \textsc{MSLR}. \textsc{MQ2008}~\footnote{https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/#!letor-4-0} consists of 800 queries with labeled query-document feature vectors. There are 46 features ranging from text based features like BM25 and network based like page rank. \textsc{MSLR}~\cite{DBLP:journals/corr/QinL13} is a bigger dataset consisting of 10k queries and 136 features per query-document vector. According to the k folder spliting, the dataset is divided into 3:1:1 as train data, validation data and test data, for the five folders of MQ2008(MSLR), the different folders just changed the order, so for all experiments we use the fold1 train-test split.

\textbf{Ranking models} We experimented with a variety of different ranking models with varied training regimens and ML algorithms. We selected models from each of the 3 main learning-to-rank categories: pointwise, pairwsie and listwise. For \textsc{MQ2008} we chose \textsc{Linear Regression} as the pointwise models, \textsc{RankNet}~\cite{burges2007learning} as the pairwise model and \textsc{LambdaMart} ~\cite{burges2010ranknet} as the listwise model. Note that \textsc{LambdaMart} is a type of GBDT (Gradient Boosted Decision Tree) that is optimized for NDCG indirectly and often used in commercial search engines. For \textsc{MSLR} we used the same pointwise and pairwise models but for the pairwise case we used \textsc{RankBoost}~\cite{freund2003efficient} instead of \textsc{RankNet} since the performance in NDCG on the test set was significantly better ($0.4$ vs $0.2$).

We used the RankLib~\footnote{https://sourceforge.net/p/lemur/wiki/RankLib/} implementation the aforementioned algorithms, there are other excellent algorithms as well.  We use the default settings of RankLib to train all models. Note that the overall performance has little bearing on our feature attribution method since we are interested in explaining a particular ordering of documents predicted rather than explaining a specific NDCG measurement.  

The trained models can be evaluated through the matrics like precision, NDCG, MAP~(Mean Average Precision), ERR~(Expected reciprocal rank).

The defination of NDCG has been explained in chapter 2, and $NDCG@k$ is the NDCG of the first k related documents retrieved. MAP represents the average accuracy rate of information retrieval systems (search engines). The formula is as follows:
\begin{equation}
MAP = \frac {\sum_{q=1}^Q AveP(q)}{Q}
\end{equation}
Among above, Q represents the number of queries, and $AveP$ represents the average accuracy rate of each query. The calculation formula is as follows:
\begin{equation}
AveP = \frac{\sum_{k=1}^n(P(k)*rel(k))}{\text{no. of total related documents}}
\end{equation}
Where k is the sorted position in the search result list, and $P(k)$ is the accuracy of the first k results, that is, $P(k) = \frac{\text{no. of total related documents}}{k}$, $rel(k)$ indicates whether the document at position k is related, 0 is not related, and 1 is related. P@k refers to the proportion of related documents in the top k retrieved documents:
\begin{equation}
precision=\frac{|\{relevant\ documents\} \cap \{retrieved\ documents\}|}{|\{retrieved\ documents\}|}
\end{equation}
ERR represents the reciprocal expectation of the stop position when the user's needs are met. The first is to calculate the probability $PP_r$ that the user stops at position r as follows:
\begin{equation}
PP_r=\prod_{i=1}^{r-1}(1-R_i)R_r
\end{equation}
where $R_i$ is a function on the relevance level of the document. The following functions can be selected:
\begin{equation}
R_i=R( g_i )=\frac{2^g - 1}{2^{g_{max}}}, g\in\{0,1,\ldots \cdots,g_{max}\}
\end{equation}
Then the calculation formula of ERR is as follows:
\begin{equation}
ERR =\sum_{r=1}^n\varphi(r)  PP_r= \sum_{r=1}^n \frac{1}{r} PP_r =\sum_{r=1}^n \frac{1}{r} \prod_{i=1}^{r-1}(1-R_i )R_r
\end{equation}
More generally, ERR does not necessarily calculate the expectation of the reciprocal of the stop position when the user's needs are met. It can be other position-based functions $\varphi(r)$, as long as $\varphi(0) = 1$ and $\varphi (r) \rightarrow 0,r\rightarrow \infty $
. For example, $\varphi (r) = \frac{1}{log_{2}\left ( r+1 \right )}$.
The command we used for evaluation looks like the following:
\begin{lstlisting}
> java -jar bin/RankLib.jar -load model.txt -test MQ2008/Fold1/test.txt -metric2T NDCG@10 -idv output/ndcg.txt
\end{lstlisting}
 where RankLib.jar is the Ranklib package. The output files (specified with -idv) provides the ndcg@10 each system achieves, here's an example of an output file showing individual and all query performance levels (in terms of the selected metric):
 \begin{lstlisting}
    NDCG@10   170   0.0
    NDCG@10   176   0.6722390270733757
    NDCG@10   177   0.4772656487866462
    NDCG@10   178   0.539003131276382
    NDCG@10   185   0.6131471927654585
    NDCG@10   189   1.0
    NDCG@10   191   0.6309297535714574
    NDCG@10   192   1.0
    NDCG@10   194   0.2532778777010656
    NDCG@10   197   1.0
    NDCG@10   200   0.6131471927654585
    NDCG@10   204   0.4772656487866462
    NDCG@10   207   0.0
    NDCG@10   209   0.123151194370365
    NDCG@10   221   0.39038004999210174
    NDCG@10   all   0.5193204478059303
\end{lstlisting}
The number after matric is the query-id, then follows NDCG@10 value for this query.

The performance of models due to page size limitation are arranged in tables ~\ref{tab:metric1},\ref{tab:metric2}, we can see the results reveal that for both datasets LambdaMART is the best model as Listwise method, while RankNet and ListNet have a poor performance, even worse than the only pointwise method Linear Regression,but as classic models, we still use them in the following experiments. For pairwise methods, MART and RankBoost are excellent models. The above results show that the results of our experiments will be very relevant to the choice of model, which is greater than the impact of the dataset.
\begin{table}[]
\begin{tabular}{c|ccccc}
\toprule
 &  & P@5 & P@10 & NDCG@5 & NDCG@10 \\ \hline
\multirow{MQ2008} & MART & 0.4162 & 0.3046 & 0.6062 & 0.41  \\  
 & RankNet & 0.2735 & 0.2368 & 0.3457 & 0.4184   \\ 
 & RankBoost & 0.3495 & 0.2751 & 0.4621 & 0.5064  \\ 
 & AdaRank & 0.3197 & 0.2426 & 0.4506 & 0.469  \\  
 & LambdaMART & 0.4534 & 0.3205 & 0.6743 & 0.6828 \\ 
 & ListNet & 0.3284 & 0.2673 & 0.4282 & 0.4807  \\
 & Linear Regression &0.3487 &0.2707 &0.4284 & 0.4714 \\
 & Random Forest & 0.3454 & 0.2759	& 0.4551&0.501 \\ \hline
\multirow{WEB10K} & MART &0.7166  &0.6756  & 0.4319 & 0.4406  \\
 & RankNet & 0.405 &0.3971  &0.1646  &0.192   \\  
 & RankBoost &0.6177  &0.5971  &0.5971  &0.3041  \\ 
 & AdaRank & 0.6662 &0.6221  &0.3784  & 0.3918  \\ 
 & LambdaMART &0.7147  & 0.6625 &0.457  &0.4687    \\ 
 & ListNet & 0.4216 & 0.4291 & 0.1449 &0.1744    \\
 & Linear Regression & 0.4523& 0.4577& 0.1916& 0.2291 \\
 & Random Forest &0.6859  &0.6511  &0.3803  &0.41   \\
\toprule
\end{tabular}
\caption{P@5, P@10, NDCG@5, NDCG@10 on RankNet, RankBoost, AdaRank, LambdaMART, ListNet, Linear Regression, Random Forest, respectively}\label{tab:metric1}
\end{table}
\newpage
\begin{table}[]
\begin{tabular}{c|cccc}
\toprule
 &  & NDCG@20 & MAP & ERR@10 \\ \hline
\multirow{MQ2008} & MART & 0.6344 & 0.6041 & 0.1292 \\  
 & RankNet &  0.4486 & 0.3741 & 0.0714 \\ 
 & RankBoost &  0.5279 & 0.4822 & 0.0979 \\  
 & AdaRank &  0.4901 & 0.4524 & 0.0735 \\  
 & LambdaMART &  0.6868 & 0.5924 & 0.1384 \\ 
 & ListNet & 0.5035 & 0.4493 & 0.0915 \\ 
 & Linear Regression &0.485 &0.4379 & 0.0952  \\ 
 & Randomforests & 0.5202 & 0.4764	& 0.0959\\ \hline
\multirow{WEB10K} & MART &  0.4903 & 0.6192 & 0.344 \\ 
 & RankNet  &0.2362  & 0.4587 & 0.1425 \\ 
 & RankBoost&0.3361  &0.3796  & 0.2282 \\ 
 & AdaRank &0.4206  & 0.5878 & 0.3077 \\ 
 & LambdaMART  &0.495  &0.608  &0.3689  \\
 & ListNet &  0.2214 &0.4654  &0.1154  \\ 
 & Linear Regression &0.2901 &0.5076 &0.1619   \\ 
 & Randomforests &0.4481  &0.6072  &0.3  \\ \toprule
\end{tabular}
\caption{NDCG@20, MAP, ERR@10 on RankNet, RankBoost, AdaRank, LambdaMART, ListNet, Linear Regression, Random Forest, respectively}\label{tab:metric2}
\end{table}


\textbf{Baseline and competitors} We use a random selection of $k$ features as the baseline for all our experiments called \textsc{Random}. The most prominent local feature attribution method in the literature is SHAP~\cite{NIPS2017_7062} (Shapley Additive Explanations). SHAP is a framework that unifies several methods that compute shapley values for input features. Shapley values are a coalitional game theoretic measure of a feature's contribution to the model output when compared to a base value, the details are in chapter 2. This base value is often the expected output of the model. We use the model agnostic implementation of the SHAP feature attribution method called Kernel Explainer~\footnote{https://github.com/slundberg/shap}. Kernel Explainer is similar to LIME~\cite{ribeiro2016i} wherein the feature attribution is calculated by estimating a local linear model that has high fidelity with the black box model, it uses a specially-weighted local linear regression to estimate SHAP values for any model.

We use SHAP for ranking models by calculating the feature attribution of the top ranked document for a given query. This approach is called \textsc{SHAP1} in our experiments. We also devise an approach called \textsc{SHAP5} that aggregates the feature attribution from SHAP for the top 5 ranked documents. The aggregation function we chose is a simple sum of the shapley values. To obtain a $k$ feature attribution explanation, we select the top-k features as per their aggregated shapley values. 

We use subprocess to call the SHAP repository. First we have to train a $Explainer$ through the training set. 
\begin{lstlisting}
 explainer = shap.KernelExplainer(score,train_feature_set)
\end{lstlisting}
Where $score$ is the function we defined to get the score through Ranklib.
Then based on this $Explainer$ we could get the shapley value matrix of based on the documents and this query.
\begin{lstlisting}
shap_values = explainer.shap_values(test_features, nsamples=200)
\end{lstlisting}
Here we do 200 samplings, the more samplings we do, the better precision we have, but it also takes more time. 
Then we pick the features which have the biggest sum of shapley values in its row. These features are the subset we want.  

The command to get scores from Ranklib could be written as:
\begin{lstlisting}
java -jar RankLib.jar -rank myResultList.txt -load myModel.txt -indri myNewRankedLists.txt
\end{lstlisting}
where myResultList.txt is the file which contains the features, myModel.txt is the model, and myNewRankedLists.txt is the output files, it will have the following appearance:
\begin{lstlisting}
1 Q0 docid=GX236-70-4445188 inc=0.600318836372593 prob=0.170566 1 3.21606 indri
1 Q0 docid=GX000-24-12369390 inc=0.60031 prob=0.416367 2 2.90221 indri
1 Q0 docid=GX016-48-5543459 inc=1 prob=0.775913 3 2.37666 indri
1 Q0 docid=GX272-12-14599061 inc=0.600318836372593 prob=0.236838 4 0.94184 indri
1 Q0 docid=GX225-79-9870332 inc=0.568969759822883 prob=0.517692 5 0.7082 indri
1 Q0 docid=GX068-48-12934837 inc=1 prob=0.659932 6 0.67412 indri
1 Q0 docid=GX265-53-7328411 inc=1 prob=0.416294 7 0.08022 indri
1 Q0 docid=GX261-90-2024545 inc=0.600318836372593 prob=0.451932 8 -0.20906 indri
\end{lstlisting}
The description can include document ID and any other information of interest to the user for the particular document. Note the input file must still contain a relevance label (the first field on a line), but it's not used since one is re-ranking a list from an already defined model, not developing the model itself or doing an evaluation. The value at the last second place in each line is the scores we need.

\textbf{Approach variants} In the previous section we presented 3 variations of our approach -- \greedy, \greedycov and \greedycovep. We also described how optimizing for validity or for completeness involves using the same procedure with only a slight modification in computing the cell values of the preference matrix. For our experiments we devise 6 variants of our approach -- 3 that optimize for completeness directly (c. \greedy, c. \greedycov and c. \greedycovep) and 3 that optimize for validity (v. \greedy, v. \greedycov and v. \greedycovep). For the comparision part of these approaches there will be some other approaches variants called \greedycovall, \greedyback and \normal. 

\textbf{Metrics} We measure the goodness of an approach in terms of validity and completeness using kendall's $\tau$. $\tau$ is computed between the original ranked list $\pi$ and a ranked list $\pi^'$  for each query in the test set. We calculate these measures exactly as defined in Chapter 4. We also compute a combined metric $\frac{c+v}{2}$ that is the mean of the completeness and validity, the detailed results will be shown in chapter 6. This simple measure is our way of determining the best overall approach. Another way is the matric called $F1$ score, as~\eqref{eqn:f1} showed, it is the harmonic mean of validity and completeness, but unfortunately some results of validity are very extreme, like 0.00001, its reciprocal will be 100000, such numbers seriously affects the effectiveness of $F1$ score. So we abandoned this metric.

\begin{equation}\label{eq:validity}
    F_1={(\frac{c^{-1}+v^{-1}}{2})}^{-1}=2\cdot \frac{v \cdot c}{v+c}
\label{eqn:f1}
\end{equation}
For \textsc{MQ2008} we take all 156 queries in the test set whereas for \textsc{MSLR} we sample 500 queries from the test set. 


For validity, we set the features $\overline{\mathcal{F}}$ to their expected value for all query-document vectors and then use $\mathcal{R}$ to compute $\pi^'$. Conversely for completeness, we set the features in $\mathcal{F}$ to their expected value and compute  $\pi^'$ as before. The expected value of each feature is the average of the feature value in the whole training set. In addition to $\tau$, we also compute $\Delta NDCG$.

\begin{equation}
\Delta NDCG = \left | NDCG(\pi ) - NDCG(\pi ^{'})\right  |
\end{equation}

The reason we use the absolute value of $\Delta NDCG$ is that we are not optimising for a worse NDCG after disturbing the selected features, what we want is changing the sort order as much as possible, no matter if NDCG get better or worse. However, it is more meaningful to calculate the ratio of $\Delta NDCG$, this is becasue that even if the value changed a little, but the $NDCG(\pi $ is also very small, so the changed can been seen as considerable. The ratio of $\Delta NDCG$ is defined in~\eqref{eqn:ratio}:
\begin{equation}
NDCG_{ratio} = \frac{\left | NDCG(\pi ) - NDCG(\pi ^{'})\right  |}{NDCG(\pi )}\label{eqn:ratio}
\end{equation}
when $NDCG(\pi)$ is 0, we take $NDCG_{ratio}$ to be 0.

In order to observe the effect of the feature attribution on the relevant documents in the list. Note that $\tau$ is not cognizant of the ground truth relevance labels. A change in rank correlation need not result in a change in NDCG. Logically, in the case of validity, increased $\tau$ and minimal $\Delta NDCG$ implies a good feature attribution method. For completeness, lower the $tau$ and higher the $\Delta NDCG$ the better the feature attribution approach. 


\section{Comparison}
In the course of our experiments, we tried a variety of approaches. Some methods differed only in some steps. Finally, we selected \greedy,\greedycov and \greedycovep. We will compare the results of these approaches with some other approaches in the following content. Specificly, we can change the stop condition of \greedycov, in \greedycov, if the row of the selected feature has no positive value, we will stop, but for \greedycovall, we continue to select features. We can also make a comparision with \greedy and the backtracking used approach called \greedyback. In addition, we can reveal the effectiveness of beam search. 

\textbf{
Why do we only care about the positive values ?}
In the score matrix we originally only care about the the cell with the positive value, in another word, we stop choosing the feature when all the values in the remaining matrix are negative. In the  table~\ref{tab:NDCG_mq2008_op} to table~\ref{tab:NDCG_MSLR_op_10} we can see the comparison about $\Delta NDCG$ between the way if only care about the positive cells and consider all cells, we named the second approach as \greedycovall. Note that in all the tables LM, RN, CA, LR, LN are abbreviations for LambdaMART, RankNet, Coordinate Ascent, Linear Regression, ListNet, respectively. Among them, Linear Regression is the pointwise method, RankNet, Coordinate Ascent are pairwise methods, and LambdaMART and ListNet are listwise methods.

\begin{table*}[]

\begin{tabular}{lcccccc}
\toprule
 & LM   & RN    & CA      & LR     & LN          \\

\midrule
                             & \multicolumn{5}{c}{Completeness} \\
\midrule                           
v. \greedycovall          &   0.1016& 	0.0413& 	0.1328& 	0.2502& 	0.0442    \\
v. \greedycov         & 0.1024& 	0.0385& 	0.1413& 	0.2555& 	0.0374   \\

\midrule
c. \greedycovall            &  0.2317& 	0.0358& 	0.148& 	0.2479& 	0.0422          \\
c. \greedycov         &  0.2212& 	0.0263& 	0.1452& 	0.2482& 	0.0288      \\

\midrule
& \multicolumn{5}{c}{Validity} \\
\midrule
v.\greedycovall           & 0.0859&	0.0735&	0.0383&	0.1047&	0.0631      \\
v. \greedycov         &0.0876&	0.0747&	0.0421&	0.1045&	0.0639       \\

\midrule
c.\greedycovall          &  0.1768&	0.0817&	0.0475&	0.1077&	0.0722       \\
c. \greedycov        &   0.1632&	0.0908&	0.044&	0.1219&	0.0754       \\
\toprule
\end{tabular}
\caption{\greedycovall  vs \greedycov, $\Delta NDCG$ on \textsc{MQ2008}  }\label{tab:NDCG_mq2008_op}
\end{table*}




\begin{table*}[]

\begin{tabular}{lcccccccc}
\toprule
 & \multicolumn{3}{c}{Validity} &  & \multicolumn{3}{c}{Completeness} \\
                             & LR   & RB   & LM && LR   & RB   & LM   \\

\midrule
v. \greedycovall          & 0.1399   & 0.1368    & 0.1873& &  0.1253  & 0.056    &  0.1112   &            \\
v. \greedycov        &  0.1282  &  0.0763   & 0.1211 &&   0.1636 &0.1048     &  0.1577   &          \\

\midrule
c. \greedycovall            & 0.1326   & 0.0794    & 0.1265&&   0.1608 &  0.1114 &   0.2063  &      \\
c. \greedycov         & 0.1388     &0.0801     &   0.1218&&  0.1624  &  0.1078   &  0.2062   &       \\

\toprule
\end{tabular}
\caption{\greedycovall  vs \greedycov, $\Delta NDCG$ on \textsc{MSLR} with 5 features }\label{tab:NDCG_mSLR_op_5}
\end{table*}


\begin{table*}[]

\begin{tabular}{lcccccccc}
\toprule
 & \multicolumn{3}{c}{Validity} &  & \multicolumn{3}{c}{Completeness} \\
& LR   & RB   & LM && LR   & RB   & LM    \\

\midrule
v. \greedycovall          &  0.1229& 0.1291&0.1775&&   0.1266&0.0592&0.1264&         \\
v. \greedycov         &  0.13&0.0565&0.103&&   0.1648 &   0.1617 &   0.2077 &       \\

\midrule
c. \greedycovall            & 0.1289&0.0552&0.1012&&  0.162&0.1645&0.2529 &  \\
c. \greedycov         &  0.1365&0.0504&0.1005&& 0.1622&0.1701&0.257&\\

\toprule
\end{tabular}
\caption{\greedycovall  vs \greedycov, $\Delta NDCG$ on \textsc{MSLR} with 10 features }\label{tab:NDCG_MSLR_op_10}
\end{table*}

We can see for dataset MQ2008, the $\Delta NDCG$ does not make much difference if we only take positive cells or not. But for MSLR, it is obviously v. \greedycov is much better, and c. \greedycov is slightly better. The situation of ratio of NDCG is same for both datasets, from table~\ref{tab:ratio_mq2008_op} to \ref{tab:ratio_MSLR_op_10} show the result.

\begin{table*}[]

\begin{tabular}{lcccccc}
\toprule
& LM   & RN    & CA      & LR     & LN           \\

\midrule
                             & \multicolumn{5}{c}{Completeness} \\
 \midrule                            
v. \greedycovall           & 0.1047 &	0.0741 &	0.2008 &	0.3585 &	0.0732      \\
v. \greedycov         &0.1062 &	0.0665 &	0.219 &	0.3647	 &0.0622 \\

\midrule
c. \greedycovall            & 0.2367 &	0.0595 &	0.2302 &	0.3575 &	0.0739          \\
c. \greedycov         & 0.2264	 &0.0429	 &0.2105 &	0.3599	 &0.0498     \\
\midrule
 & \multicolumn{5}{c}{Validity} \\
\midrule
v. \greedycovall           & 0.0882 &	0.1226 &	0.0602 &	0.1614 &	0.11          \\
v. \greedycov         &  0.0904 &	0.1215	 &0.069	 &0.1595 &	0.1091    \\

\midrule
c. \greedycovall           &   0.181 &	0.1326	 &0.0704 &	0.1639 &	0.1301   \\
c. \greedycov        &   0.1667 &	0.148 &	0.0648	 &0.1888 &	0.1374        \\
\toprule
\end{tabular}
\caption{\greedycovall  vs \greedycov, NDCG ratio on \textsc{MQ2008}}\label{tab:ratio_mq2008_op}
\end{table*}




\begin{table*}[]

\begin{tabular}{lcccccccc}
\toprule
 & \multicolumn{3}{c}{Validity} &  & \multicolumn{3}{c}{Completeness} \\
& LR   & RB   & LM && LR   & RB   & LM    \\

\midrule
v. \greedycovall          &0.9408&0.455&0.3945 & &  0.7403&0.1889&0.2484 &      \\
v. \greedycov         & 0.7681&0.2713&0.2769&& 0.8145& 0.3241&0.3257  &        \\

\midrule
c. \greedycovall            & 0.8398&0.2782&0.2959&&0.8388&0.3531&0.2063 &   \\
c. \greedycov         &  0.8013&0.2651&0.2828&& 0.8268&0.3352&0.4229  &    \\

\toprule
\end{tabular}
\caption{\greedycovall  vs \greedycov,  NDCG ratio on \textsc{MSLR} 5 features}\label{tab:ratio_MSLR_op_5}
\end{table*}



\begin{table*}[]

\begin{tabular}{lcccccccc}
\toprule
 & \multicolumn{3}{c}{Validity} &  & \multicolumn{3}{c}{Completeness} \\
& LR   & RB   & LM && LR   & RB   & LM    \\

\midrule
v. \greedycovall           &  0.7878&	0.438&	0.3892&& 0.6172&	0.2047&	0.268  &          \\
v. \greedycov         &   0.7906  &	0.1973  &	0.2323&&   0.8133	&0.4921	&0.4106   &     \\

\midrule
c. \greedycovall       &     0.8495&  	0.4907&  	0.2529&&0.8308&0.1843&0.2568&     \\
c. \greedycov        &   0.8456&  	0.5361&  	0.5157 && 0.8013& 0.2651 &0.2986&  \\

\toprule
\end{tabular}
\caption{\greedycovall  vs \greedycov,  NDCG ratio on  \textsc{MSLR} 10 features}\label{tab:ratio_MSLR_op_10}
\end{table*}


\begin{table*}[]

\begin{tabular}{lcccccc}
\toprule
 & LM   & RN    & CA      & LR     & LN          \\

\midrule
                             & \multicolumn{5}{c}{Completeness} \\
\midrule                            
v. \greedycovall           & -0.0958& 	-0.371& 	-0.0531& 	0.0724& 	-0.3681        \\
v. \greedycov         & -0.0821& 	-0.3792& 	-0.0353& 	0.0616& 	-0.3951      \\

\midrule
c. \greedycovall            & -0.0047& 	-0.3455& 	-0.0298& 	0.0488& 	-0.3581           \\
c. \greedycov         & -0.0138	& -0.5303& 	-0.0438& 	0.0628& 	-0.5209        \\
\midrule
 & \multicolumn{5}{c}{Validity} \\
\midrule
v. \greedycovall           & 0.2701& 	0.2715& 	0.4148& 	0.1924& 	0.2657          \\
v. \greedycov         & 0.2808& 	0.2678	& 0.4149& 	0.1978& 	0.2892      \\

\midrule
c. \greedycovall           &  0.0828& 	0.2626& 	0.3909& 	0.1083& 	0.2747        \\
c. \greedycov        &  0.0631& 	0.2541& 	0.3819& 	0.1097& 	0.2274          \\
\toprule
\end{tabular}
\caption{\greedycovall  vs \greedycov,  $\tau$ on \textsc{MQ2008}}\label{tab:tau_mq2008_op}
\end{table*}




\begin{table*}[]

\begin{tabular}{lcccccccc}
\toprule
 & \multicolumn{3}{c}{Validity} &  & \multicolumn{3}{c}{Completeness} \\
 & LR   & RB   & LM && LR   & RB   & LM   \\

\midrule
v. \greedycovall           &0.0709    &0.0939     &   0.081&& -0.025   & -0.13    &  -0.0699   &            \\
v. \greedycov           & 0.0345   &  0.0696   &   0.0588   &&  0.0024  & -0.0647    &   -0.0396  &      \\

\midrule
c. \greedycovall           &  0.0116  &  0.0586   &   0.0308& &  0.0068  &   -0.0624  &  -0.0246   &     \\
c. \greedycov         &  0.0111  &    0.0521 &  0.0249  &&  0.0026  &  -0.0581   &    -0.0197 &   \\

\toprule
\end{tabular}
\caption{\greedycovall  vs \greedycov,  $\tau$ on \textsc{MSLR} with 5 features}\label{tab:tau_mSLR_op_5}
\end{table*}



\begin{table*}[]

\begin{tabular}{lccccccc}
\toprule
 & \multicolumn{3}{c}{Validity} &  & \multicolumn{3}{c}{Completeness} \\
& LR   & RB   & LM && LR   & RB   & LM   \\

\midrule
v. \greedycovall      & 0.0848   & 0.1041  &  0.0792  && -0.0173&	-0.1251&	-0.1251 \\
v. \greedycov         &   0.0401 &   0.1078  & 0.0634  &&  0.001&	-0.0198&	-0.0198	    \\

\midrule
c. \greedycovall            & 0.0132   & 0.0911    & 0.0332&& -0.1251&	-0.1251&	-0.0158      \\
c. \greedycov         &   0.0147 &0.0951     &    0.0376&&  -0.0198&	-0.0198&	-0.0131    \\

\toprule
\end{tabular}
\caption{\greedycovall  vs \greedycov,  $\tau$ on \textsc{MSLR} with 10 features}\label{tab:tau_mSLR_op_10}\end{table*}

But if we take a look at Kendall's $\tau$ from table~\ref{tab:tau_mq2008_op} to ~\ref{tab:tau_mSLR_op_10}, we can find that the c. \greedycovall's results is better than c. \greedycov on MQ2008, but for validity optimized approaches are almost same. For MSLR, completeness optimized approaches have similar results. Interesting is that for  v. \greedycovall do better on validity but v. \greedycov on completeness results.

\textbf{ Should we do backtracking ?}
From table~\ref{tab:NDCG_mq2008_backtracking} to table ~\ref{tab:tau_mq2008_backtracking} we compare the result of the backtracking approach and \greedy, we note the backtracking approach here as \greedyback. Remember that for \greedy, we stop selecting features when the positive cells is getting fewer in the row of last selected features. The difference for backtracking way is that set a patience to 1 here, which means for the first feature, if the positive cells gets fewer we take the second candidate feature from the beam step. Because we optimise for $\tau$ in the beam search, so the results of $\Delat NDCG$ and NDCG ratio for the new method have not changed a lot, but for $\tau$ in table~\ref{tab:tau_mq2008_backtracking} and ~\ref{tab:tau_mq2008_backtracking}, the new method is better.

\begin{table*}[]
\begin{tabular}{lcccccc}
\toprule
  & LM   & RN    & CA      & LR     & LN       \\

\midrule
                             & \multicolumn{5}{c}{Completeness} \\
 \midrule                         
v. \greedy            &0.1236&	0.0397&	0.1365&	0.2684&	0.0354        \\
v. \greedyback         &  0.1107&	0.0336&	0.1271&	0.2681&	0.0421        \\

\midrule
c. \greedy            & 0.2656&	0.0582&	0.1341	&0.2676&	0.0381     \\
c. \greedyback         &   0.2541&	0.036&	0.1327&	0.2567&	0.0403  \\
\midrule
& \multicolumn{5}{c}{Validity} \\
\midrule
v. \greedy            & 0.0826& 	0.0755& 	0.038& 	0.081& 	0.056   \\
v. \greedyback         & 0.0757& 	0.0796& 0.046& 	0.0844& 	0.0703    \\

\midrule
c. \greedy            &  0.132& 	0.0804& 	0.0441& 	0.0829& 	0.0705       \\
c. \greedyback        &  0.1528& 	0.0799& 	0.0404	& 0.0849& 	0.0653      \\
\toprule
\end{tabular}
\caption{\greedy vs \greedyback, $\Delta NDCG$ on \textsc{MQ2008}}\label{tab:NDCG_mq2008_backtracking}
\end{table*}





\begin{table*}[]

\begin{tabular}{lcccccc}
\toprule
& LM   & RN    & CA      & LR     & LN       \\

\midrule
                             & \multicolumn{5}{c}{Completeness} \\
\midrule                             
v. \greedy            &  0.1272	&0.0683&	0.2011&	0.3775&	0.061       \\
v. \greedyback         &  0.114&	0.0556&	0.1942&	0.3833&	0.0712     \\

\midrule
c. \greedy            &    0.2713&	0.344&	0.2048	&0.3777&	0.0676 \\
c. \greedyback         & 0.2596&	0.0595	&0.2022	&0.3589	&0.0703      \\
\midrule
& \multicolumn{5}{c}{Validity} \\
\midrule
v. \greedy            & 0.1272& 	0.1267& 	0.058& 	0.1298& 	0.1066   \\
v. \greedyback         & 0.0782& 	0.1316& 	0.0704& 	0.138& 	0.1225     \\

\midrule
c. \greedy            &    0.1353& 	0.1315& 	0.0645& 	0.13& 	0.1271     \\
c. \greedyback        &  0.1563	& 0.1307& 	0.0658& 	0.1276& 	0.1182        \\
\toprule
\end{tabular}
\caption{\greedy vs \greedyback, NDCG ratio on \textsc{MQ2008}}\label{tab:ratio_mq2008_backtracking}

\end{table*}




\begin{table*}[]

\begin{tabular}{lcccccc}
\toprule
 & LM   & RN    & CA      & LR     & LN       \\

\midrule
                             & \multicolumn{5}{c}{Completeness} \\
 \midrule                          
v. \greedy            &  -0.0919&  	-0.3693&  	-0.0476&  	0.0418&  	-0.3823        \\
v. \greedyback         & -0.0758&  	-0.3677&  	-0.0509&  	0.0428	&-0.3678          \\

\midrule
c. \greedy            &0.0131&  	-0.0804&  	-0.0437&  	0.0228&  	-0.3538       \\
c. \greedyback         & -0.0136&  	-0.343&  	-0.0554&  	0.0386&  	-0.3509        \\
\midrule
 & \multicolumn{5}{c}{Validity} \\
\midrule
v. \greedy            &  0.2592	 &  0.2872	 &  0.4358	 &  0.157 &  	0.2901    \\
v. \greedyback         &   0.2729 &  	0.2835 &  	0.4455 &  	0.1456 &  	0.27     \\

\midrule
c. \greedy                &   0.1397	 &  0.2696 &  	0.4481 &  	0.1673 &  	0.2844 \\
c. \greedyback       &    0.1226 &  	0.27 &  	0.4419 &  	0.1509 &  	0.2718           \\
\toprule
\end{tabular}
\caption{\greedy vs \greedyback, $\tau$ on \textsc{MQ2008}}\label{tab:tau_mq2008_backtracking}
\end{table*}




\textbf{ Should we do beam search?}
The reason why we do beam search at the first step is that really great improvement has been achieved. We also do experiments without beam search, from table ~\ref{tab:NDCG_mq2008_without} to ~\ref{tab:tau_mq2008_without} we can see that beam search is much better than the normal way, the way which do not do beam search is named as \normal here.





\begin{table*}[]

\begin{tabular}{lccccccc}
\toprule
                             & \multicolumn{6}{c}{Completeness} \\
                             & LM  & RN   & CA     &LR     & LN  & RF     \\
\midrule
v. \normal            & 0.0209 & 	0.0761 & 	0.0808 & 	0.1545 & 	0.0645 & 	0.0572  \\
v. \greedycov    &0.1016 & 	0.0413 & 	0.1328 & 	0.2502 & 	0.0442&0.0492	   \\

\midrule
c. \normal            &0.2123 & 	0.0365 & 	0.0406	 & 0.2529 & 	0.1378 & 	0.0908     \\
c. \greedycov  &   0.2317 & 	0.0358 & 	0.148 & 	0.2479	 & 0.0422	 & 0.0914      \\
\midrule
 & \multicolumn{6}{c}{Validity} \\
\midrule
v. \normal            &  0.3304&	0.0755&	0.0454	&0.1977	&0.0695	&0.2455 \\
v. \greedycov         & 		0.0859&	0.0735&	0.0383&	0.1047&	0.0631&0.0762  \\

\midrule
c. \normal            & 	0.1738&	0.0825&	0.0406	&0.1197&	0.0737&	0.0875 \\
c. \greedycov         &  0.1768&	0.0817&	0.0475	&0.1077	&0.0722&	0.0844  \\

\toprule
\end{tabular}
\caption{\normal vs \greedycov, $\Delta NDCG$ on \textsc{MQ2008}} \label{tab:NDCG_mq2008_without}
\end{table*}


\begin{table*}[]

\begin{tabular}{lccccccc}
\toprule
 & LM   & RN    & CA      & LR     & LN & RF     \\
\midrule
                             & \multicolumn{6}{c}{Completeness} \\
\midrule                           
v. \normal            &0.0335&	0.1756&	0.2109&	0.2391&	0.1048&	0.0779 \\
v. \greedycov    &0.1047&	0.0741&	0.2008&	0.3585&	0.0732&0.073	 \\

\midrule
c. \normal            & 0.217&	0.0606&	0.0612&	0.3655&	0.0655&	0.1244   \\
c. \greedycov  &  0.2367&	0.0595&	0.2302&	0.3575&	0.0739&	0.1251        \\
\midrule
  & \multicolumn{6}{c}{Validity} \\
\midrule
v. \normal            &0.3394&	0.0889&	0.0679&	0.2856&	0.1154&	0.3041    \\
v. \greedycov         & 0.0882&	0.1226&	0.0602&	0.1614&	0.11&	0.1081  \\

\midrule
c. \normal            & 0.1776	&0.1334&	0.0612&	0.1809&	0.1302&	0.1187\\
c. \greedycov         &0.181&	0.1326&	0.0704	&0.1639&0.1301	&0.1055 \\
\toprule
\end{tabular}
\caption{\normal vs \greedycov, NDCG ratio on \textsc{MQ2008}  }
\end{table*}\label{tab:ratio_mq2008_without}



\begin{table*}[]

\begin{tabular}{lccccccc}
\toprule
                             & LM   & RN    & CA      & LR     & LN  & RF     \\
\midrule
                             & \multicolumn{6}{c}{Completeness} \\
\midrule
v. \normal            & -0.6114& 	-0.0784& 	-0.1326& 	-0.0662& 	-0.2047& 	-0.2802  \\
v. \greedycov    &		-0.0958& 	-0.371& 	-0.0531& 	0.0724& 	-0.3681&-0.2722   \\

\midrule
c. \normal            & -0.0231	& -0.3472& 	-0.3724& 	0.0651& 	-0.3457& 	-0.0961    \\
c. \greedycov  &      -0.0047& 	-0.3455& 	-0.0298& 	0.0488	& -0.3581& 	-0.096      \\
\midrule
 & \multicolumn{6}{c}{Validity} \\
\midrule
v. \normal            &  	0.0261& 	0.4285& 	0.466& 	-0.0028& 	0.2706& 	-0.0021  \\
v. \greedycov         & 		 0.2701& 	0.2715& 	0.4148& 	0.1924& 	0.2657& 0.4352   \\

\midrule
c. \normal            & 	0.0835& 	0.2551& 	0.3724& 	0.1058& 	0.259& 	0.1935 \\
c. \greedycov         &  0.0828	& 0.2626& 	0.3909& 	0.1083& 	0.2747& 	0.1991  \\

\toprule
\end{tabular}
\caption{\normal vs \greedycov, $\tau$ on \textsc{MQ2008}} \label{tab:tau_mq2008_without}
\end{table*}





